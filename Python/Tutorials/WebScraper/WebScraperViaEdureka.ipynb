{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f149905-4c03-41c9-a316-bcc0da406ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-d55e93284157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'_1LKTO3'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#    print (b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Name: Abel\n",
    "#Date: 2015.07.15\n",
    "#Purpose: To scrape laptop data from flipkart and display in .xlsx\n",
    "#Source: heavily modified from https://www.edureka.co/blog/web-scraping-with-python/\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "products=[] #List to store name of the product\n",
    "prices=[] #List to store price of the product\n",
    "ratings=[] #List to store rating of the product\n",
    "URL = \"https://www.flipkart.com/laptops/pr?sid=6bo,b5g&otracker=categorytree&fm=neo%2Fmerchandising&iid=M_2d85f06f-090b-4193-8d9a-756578561593_1_372UD5BXDFYS_MC.34WHNYFH5V2Y&otracker=hp_rich_navigation_7_1.navigationCard.RICH_NAVIGATION_Electronics~Laptop%2Band%2BDesktop_34WHNYFH5V2Y&otracker1=hp_rich_navigation_PINNED_neo%2Fmerchandising_NA_NAV_EXPANDABLE_navigationCard_cc_7_L1_view-all&cid=34WHNYFH5V2Y\"\n",
    "\n",
    "for x in range(6):\n",
    "    result = requests.get(URL)\n",
    "    #print (result.status_code) #sanity check to ensure that the site exists, should return 200\n",
    "    #print (result.headers)\n",
    "    src = result.content #saves the entire source code...\n",
    "    soup = BeautifulSoup(src)\n",
    "    #for a in soup.findAll('div',href=True, attrs={'class':'_1AtVbE col-12-12'}): #backup\n",
    "    for a in soup.findAll('div', attrs={'class':'_1AtVbE col-12-12'}):\n",
    "        #print (a)\n",
    "        #name=a.find('div', attrs={'class':'_4rR01T'})\n",
    "        if a.find('div', attrs={'class':'_4rR01T'}) == None:\n",
    "            continue\n",
    "        name=a.find('div', attrs={'class':'_4rR01T'})\n",
    "        #print(name)\n",
    "        price=a.find('div', attrs={'class':'_30jeq3 _1_WHN1'})\n",
    "        rating=a.find('div', attrs={'class':'_3LWZlK'})\n",
    "        #print(rating)\n",
    "        products.append(name.text)\n",
    "        prices.append(price.text)\n",
    "        ratings.append(rating.text) \n",
    "    for b in soup.findAll('a', attrs={'class':'_1LKTO3'}):\n",
    "    #    print (b)\n",
    "        if b.text == \"Next\":\n",
    "            #nextURL = soup.find_all('a', attrs={'class':'_1LKTO3'})[-1].attrs['href']\n",
    "            #URL = \"https://www.flipkart.com\" + nextURL\n",
    "            URL = \"https://www.flipkart.com\" + b.attrs['href']\n",
    "    print(f\"Next URL: {URL}\")\n",
    "    \n",
    "#df = pd.DataFrame({'Product Name':products,'Price':prices,'Rating':ratings}) \n",
    "#print (df.shape)\n",
    "#df.head(10)\n",
    "#print (df.reset_index())\n",
    "#df.to_csv('products.csv', index=False, encoding='utf-8') #to make a .csv file containing this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e689eff-82a8-4a7f-aee7-dc65de183f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
